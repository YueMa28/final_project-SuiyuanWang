---
title: "Mapping fine-scale population using dasymetric method"
author: Suiyuan Wang
subtitle: A case study in Buffalo city, NY
---

# Introduction

Knowledge of the fine-level population distribution is vital for measuring the impacts of socio-economic justice, optimizing resource allocation, and assessing the risks of environmental exposures. The traditional way to get population data is from the census bureau. However, population data released by the census bureau are too coarse and irregular to satisfy the requirements of accurate analysis. To solve the irregular shape and too coarse level problem of census data, we would downscale the census block group level data to census block. As for the method, the dasymetric method would be applied.  The dasymetric method uses locality knowledge to depict the uneven population distribution within zones without assuming even distributions. This volume preserve method disaggregates the population within the source zone to the target zones according to the weighted layer generated by population-related ancillary information to preserve the total population count. The dasymetric method has been evaluated as an effective and feasible way for population disaggregation. Finally, the result will be evaluated comparing with census block data. Overall, this project aims to apply the dasymetric method to disaggregate census data to get fine-level population data using open-source data.

# Materials and methods

```{r, message=F, warning=F}
require(sf)
require(dasymetric)
library(tidyverse)
library(tidycensus)
library(tigris)
options(tigris_use_cache = TRUE)
library(leaflet)
library(kableExtra)
knitr::opts_chunk$set(cache=TRUE)  # cache the results for quick compiling
```

1. Download data 
* Census boundaries (2020) with population. 
 * Census block population_block
 * Census block group population_bg
* Microsoft building footprint (2018).
* Buffalo zoning data
```{r}
# Buffalo city zoning data Landuse.
landuse <- read_sf("data/zoning.shp") 

## definition
source_geom <- read_sf("data/census_bg.shp")
target_geom <- read_sf("data/census_block.shp")

# ancillary data
bf <- read_sf("data/building.shp")

plot(source_geom["population_bg"],breaks = c(0,5000,10000,15000,20000,25000,30000,35000,40000,45000), main="Buffalo city Population at Census Block Group (2020)")
```

2. Clean required data
```{r dasymetric, warning=FALSE, message=FALSE}
###filter landuse as residential, using residential landuse to filter building footprint
residential = prep_landuse(landuse)
residential_bf = filter(bf, residential)
```

```{r dasymetric, warning=FALSE, message=FALSE}
# dasymetric map with landuse information as ancillary data
dm_pop = dasymetric_map(target_geom, source_geom, residential_bf, extensive = "population_b")
```
6. evaluating the result by integrating the grids data into census block level, and plot R2.
[REFER TO] (https://lukemiller.org/index.php/2012/10/adding-p-values-and-r-squared-values-to-a-plot-using-expression/)
```{r}
#calculate r2 in dm_pop, 'population_b' & 'population_bg'
mod1 = lm(population_b~population_bg, data = dm_pop)
modsum = summary(mod1)
plot(dm_pop$population_b, dm_pop$population_bg, pch = 20, type = 'p', las = 1,
		xlab = expression(paste('Census block Population')),
		ylab = 'Estimate population')

#adding the regression line from the linear model
abline(mod1)
```

```{r}
r2 = modsum$adj.r.squared
```

```{r}
modsum$coefficients
```

```{r}
my.p = modsum$coefficients[2,4]
mylabel = bquote(italic(R)^2 == .(format(r2, digits = 3)))
text(x = 19, y = 2.5, labels = mylabel)
```

```{r}
mylabel = bquote(italic(R)^2 == .(format(r2, digits = 3)))
text(x = 19, y = 2.5, labels = mylabel)
legend('topright', legend = rp, bty = 'n')
```


7. mapping.
```{r}
plot(dm_pop["pop_sum"],breaks = c(0,5000,10000,15000,20000,25000,30000,35000,40000,45000),main="Dasymetric Population Map based on landuse and building footprint(2020)")
```

## Download and clean all required data
```{r}
n=20
data=data.frame(x=runif(n,-180,180),
                y=runif(n,-60,60),
                size = runif(n, 5, 20),
                category = factor(
                  sample(letters[1:5], n, replace = TRUE)
                  ),
                value = rnorm(n))
```

```{r, results='asis'}
data %>% 
  slice(1:10) %>% #show only 1:n rows
  kable(digits=2,align="c")%>% #make table and round to two digits
  kable_styling(bootstrap_options = 
                  c("striped", "hover", "condensed", "responsive")) #apply other formatting
```

Add any additional processing steps here.

# Results

[~200 words]

Tables and figures (maps and other graphics) are carefully planned to convey the results of your analysis. Intense exploration and evidence of many trials and failures. The author looked at the data in many different ways before coming to the final presentation of the data.

Show tables, plots, etc. and describe them.

```{r, fig.width=6, fig.height=3, fig.cap="Map of completely random data"}
m <- leaflet(data) %>% 
  addTiles() %>% 
  addCircleMarkers(~x, ~y, radius = ~size,color = ~as.factor(category)) %>% 
  addPopups(~x[2], ~y[2], "Random popup")
m  # a map with the default OSM tile layer
```




# Conclusions

[~200 words]

Clear summary adequately describing the results and putting them in context. Discussion of further questions and ways to continue investigation.

# References

All sources are cited in a consistent manner





Two key packages: `sf` for vectors and `raster` for grid.
Additional packaegs discussed: `ggplot2`, `tmap`, `dplyr`

read in vector data with the `sf` package and st_read().
read in raster data using the raster package and either the raster()or brick() functions.
raster(): single band raster
brick(): multi-band raster.
write your data:
vector
st_write(my_poly, "data/my_poly.shp")
raster
writeRaster(my_raster, "data/my_raster.tif")
